---
title: "class 08 mini project"
author: "Lawrence Adhinatha"
date: "2023-02-02"
output: pdf_document
---

In today's mini-project we will explore a complete analysis using the unsupervised learning techniques covered in class (clustering and PCA for now). 

The data itself comes from the Wisconsin Breast Cancer Diagnostic data set FNA biopsy data. 

# Exploratory Data Analysis

## Data import
```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)
```

Remove the diagnosis column and keep it in a separate vector for later. 
```{r}
diagnosis <- as.factor(wisc.df[,1])
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```
There are 569 cells observed in this dataset. 

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```
212 cells have a malignant diagnosis. 

> Q3. How many variables/features in the data are suffixed with _mean?

First find the column names
```{r}
colnames(wisc.data)
```
Next, I need to search within the column names for "_mean" pattern The `grep()` function might be of use. 
```{r}
length(grep("_mean", colnames(wisc.data)))
```
There are 10 features in the data suffixed with _mean. 

> Q. How many dimensions are in this dataset? 

```{r}
ncol(wisc.data)
```

# Principal Component Analysis 

First, check if the variables are all on the same scale: Do we need to scale the data before performing PCA? 
```{r}
round(apply(wisc.data, 2, sd), 3)
```
Looks like we need to scale. 

```{r}
# Perform PCA on wisc.data 
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs (PC3)

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs (PC7)

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

This plot is difficult to understand, as the points are too close together to see any meaningful correlation. 
```{r}
biplot(wisc.pr)
```


```{r}
# Plot PC2 vs PC1
plot(wisc.pr$x[,1:2], col=diagnosis, xlab="PC1", ylab="PC2")
```
> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,c(1, 3)], col=diagnosis, xlab="PC1", ylab="PC3")
```
These plots display a distinction between two subgroups. There is a relatively clean separation between Benign and Malignant cell groups in the PC2 vs PC1 plot, while there is a less clean but still visible distinction in the PC3 vs PC1 plot. This is due to the fact that PC2 explains more variance in the data than PC3. 

We can also create a plot using ggplot: 
```{r}
library(ggplot2)
wisc.df <- as.data.frame(wisc.pr$x)
ggplot(wisc.df, aes(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)) +
  geom_point() +
  labs(x="PC1", y="PC2")
```

Making scree plots to show how much variance is explained as the PC number increases: 

First, store the variance in a variable.
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Next, calculate the variance described by each PC vs that of all PCs.
```{r}
pve <- (wisc.pr$sdev^2)/sum(wisc.pr$sdev^2)
plot(pve, 
     xlab="Principal Component", ylab="Proportion of Variance Explained", type="o")
```

We can also use a barplot to represent this variance; this time we'll use the CRAN package factoextra. 

```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels=TRUE)
```

# Examine the PC loadings

How much do the original variables contribute ot the new PCs that we have calculated? to get at the is at we can look at the `$rotation` componentn of the returned PCA object. 
```{r}
head(wisc.pr$rotation[,1:3])
```
Focus in on PC1

```{r}
head(wisc.pr$rotation[,1])
```
> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean",1]
```

There is a complicated mix of variables that go together to make up PC1 (i.e. there are many of the original variables that together contribute highly to PC1).
```{r}
loadings <- as.data.frame(wisc.pr$rotation)

ggplot(loadings) +
  aes(PC1, rownames(loadings)) + 
  geom_col()
```

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
summary(wisc.pr)
```
5 PCs (PC5)

# Hierarchical Clustering

Scale the data
```{r}
data.scaled <- scale(wisc.data)
```

Calculate Euclidean distances between all pairs of observations
```{r}
data.dist <- dist(data.scaled)
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
wisc.hclust <- hclust(data.dist, )
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```
A height of 19 yields 4 clusters for the model. 


Cut this tree to yield cluster membership vector using `cutree()` function. 

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h=19)
table(wisc.hclust.clusters)
```
```{r}
table(wisc.hclust.clusters, diagnosis) 
```
> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10? 

```{r}
wisc.hclust.clusters.4 <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters.4, diagnosis)
```
The best cluster vs diagnosis match is at k=4 clusters, as it is the lowest number of groupings necessary to separate the benign and malignant groups accurately enough, before the increase in accuracy tapers off as the value of `k` increases. 

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

```{r}
methods <- function(x) {
  table(cutree(hclust(data.dist, method=x), k=4), diagnosis)
}

methods("single")
methods("complete")
methods("average")
methods("ward.D2")
```
`method="complete"` and `method="ward.D2"` give my favorite results.  `"ward.D2"` generally separates B and M into two groups, with some overlap and increased clustering in other groups; `"complete"` in this case is the best method, since it keeps clustering mostly within two groups, 1 and 3, and separates B and M about as well as `"ward.D2"`. In contrast, `"single"` and `"average"` both leave nearly all the data points in one group, failing to separate the diagnoses at all.  


# K-means clustering
```{r}
wisc.km <- kmeans(data.scaled, centers=2, nstart=50)
table(wisc.km$cluster, diagnosis)
```
> Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

K-means does a good job of separating the two diagnoses, producing very similar results to the hclust results. There is a consistently high amount of false positives and a lower amount of false negatives in both models. 

Comparing clusters using k-means model with clusters using hierarchical clustering model: 
```{r}
table(wisc.hclust.clusters, diagnosis)
```


# Combine methods: PCA and HCLUST 

My PCA results were interesting as they showed a separation of M and B samples along PC1. 
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

I want to cluster my PCA results - that is use `wisc.pr$x` as input to `hclust`. 

Try clustering in 3 PCs, that is PC1, PC2, PC3 as input 
```{r}
d <- dist(wisc.pr$x[,1:3]) 

wisc.pr.hclust <- hclust(d, method="ward.D2")
```

Tree result figure: 
```{r}
plot(wisc.pr.hclust)
```

Let's cut this tree into two groups/clusters
```{r}
wisc.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.hclust.clusters)
```

```{r}
plot(wisc.pr$x[,1:2], col=wisc.hclust.clusters)
```

```{r}
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=wisc.hclust.clusters)
rglwidget(width = 400, height = 400)
```


How well do the two clusters separate the M and B diagnosis? 
```{r}
table(wisc.hclust.clusters, diagnosis) 
```
Calculate the accuracy of our results
```{r}
(179+333)/nrow(wisc.data)
```
Nearly 90% of our results are accurate. However, a 10% false positive could potentially be dangerous to patients. 


> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
wisc.pr.hclust.clusters.4 <- cutree(wisc.pr.hclust, k=4)
table(wisc.pr.hclust.clusters.4, diagnosis)
```
With four clusters, this new model produces a group with too much of each diagnosis to be as useful as with 2 clusters. 

> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
table(wisc.km$cluster, diagnosis)
wisc.pr.hclust.clusters.2 <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters.2, diagnosis)
```
The k-means and hierarchical clustering combination separated the diagnoses with similar accuracy as the PCA clustering model. 


# Sensitivity and specificity 

> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

```{r}
# function for sensitivity and specificity calculations
sens.spec <- function(tp, fn, tn, fp) {
  sens <- tp/(tp+fn)
  spec <- tn/(tn+fp)
  results <- c(sens, spec)
  names(results) <- c("Sensitivity", "Specificity")
  results
}

# results for each method 
## table(wisc.hclust.clusters, diagnosis)
hclust <- sens.spec(165, 12, 343, 40)

## table(wisc.km$cluster, diagnosis)
kmeans <- sens.spec(175, 14, 343, 37)

## table(wisc.pr.hclust.clusters.2, diagnosis)
pca <- sens.spec(179, 24, 333, 33)

# data frame of all method results
sens.spec.df <- data.frame(hclust, kmeans, pca)
sens.spec.df

# calculate colnames of max sensitivity and max specificity
sens.spec.df$max <- colnames(sens.spec.df)[apply(sens.spec.df, 1, which.max)]
best <- sens.spec.df$max
names(best) <- c("Best Sensitivity", "Best Specificity")
best
```

# Prediction

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```
```{r}
plot(wisc.pr$x[,1:2], col=wisc.hclust.clusters)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q18. Which of these new patients should we prioritize for follow up based on your results?

We should prioritize patient 2, as their cell data seem to correspond to the PCA group denoting a malignant diagnosis. 















